
Ask user to write a class called JobConfiguration. It has one function called setup() that returns a JobConf object.
The user creates a new JobConf object in that function, sets up everything and returns that JobConf object.

Then user goes to master cmd line and starts a job by typing "hdstart <input file path> <output dir> <dir of mapper, reducer and configuration files>

Upon start up, master loads the JobConfiguration class, and gets the JobConf object. Then it does sanity checks on 
the setup in JobConf object. If everything passes, it starts the job in a new thread, and waits for new jobs on cmd 
line. A better idea would be to have setup() return an array of JobConf objects to chain jobs, ordered by the 
order in which they want them to run.

----------------------

* Create folders for all reducers before hand - Done
* Make sure partition folders are empty before each job - Done for now commented out.
* Reducer should only create part files when needed - Done
* Fault tolerance 
* Threads to Jobs - Partially done except for extracting shared maps back to the master
* Add threads to worker - Done Seems to be working for now. Check and make sure ?
* Add job tracker thread to master 


 javac -cp json-simple-1.1.1.jar ./*/*.java
 
Master 
	java -classpath .:json-simple-1.1.1.jar master/HadoopMasterNew
and then to run the job
	RUNJOB test.JobConfiguration words.txt

Worker 
	 java -classpath .:json-simple-1.1.1.jar nodework/Worker 9045
	 
	 
Why is max port 49151 ?



File writes:

* Each map worker writes output to jobname_worker/workeri.txt
* Partitioner reads worker files and writes to jobname_partition/reducer_i.txt
* Reducer reads its file and writes to final_answers/part_i.txt

If we call quit on worker, and worker exits, how is that taken care of in the master?
Handout says, we should be able to restart processes if they fail
Improve management tools (job time started, ended, list of working workers, etc.)
Check if we are still using PING messages
If the workers are not online, the chunk maps aren't cleaned up. Should clean those up.
Class loader shit. And forcing the name of job config class. Check this.




Fault Tolerance 

We provide for the fault tolerance of the workers dying while doing either a map or a reduce task.
With a configurable mapper timeout ( which the user can change as per his convenience ) we make 
the master job thread resend the chunk to the worker. We can do this because we have the 
worker -> chunk and chunk -> worker mapping which allows us to re send a map chunk that has
exceeded the known timeout limit. 

Similarly when the workers are in the middle of reduce phase if a worker dies then we take
his reduce task and give it to another worker if the time has exceeded the reducer timeout limit.
 
 






